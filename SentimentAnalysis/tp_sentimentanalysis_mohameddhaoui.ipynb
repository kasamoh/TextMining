{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> TP analyse des opinions dans les Tweets </center>##\n",
    "#### <center> Elaboré par : Mohamed DHAOUI  - MS Big Data  </center>####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L’objectif de ce TP est d’analyser un corpus de tweets en fonction des opinions exprimées (positif/-\n",
    "négatif). Le langage à utiliser est Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La base des tweets à analyser contient 498 tweets annotés manuellement. La base propose 6 champs correspondant aux informations suivantes :\n",
    "1. la polarité du tweet : Chaque tweet est accompagné d’un score pouvant être égal à 0 (négatif), 2\n",
    "(neutre) ou 4 (positif).\n",
    "2. l’identifiant du tweet (2087)\n",
    "3. la date du tweet (Sat May 16 23 :58 :44 UTC 2009)\n",
    "4. la requête associée (lyx). Si pas de requête la valeur est NO_ QUERY.\n",
    "5. l’utilisateur qui a tweeté (robotickilldozr)\n",
    "6. le texte du tweet(Lyx is cool)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importation des packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf8\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk as nltk\n",
    "import re\n",
    "from nltk.corpus import wordnet as wn\n",
    "import csv\n",
    "\n",
    "\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.decomposition.pca import PCA\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from gensim.models import Word2Vec\n",
    "import html.entities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1- Prétraitements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cette partie , on va essayer de nettoyer les tweets et supprimer les caractères spéciaux susceptibles de nuire à la mise en place des méthodes d’analyse d’opinions. Pour ce faire , on va commencer par récuperer le fichier de tweet , ajouter les noms de colonnes et ensuite faire les étapes suivantes : \n",
    "- récupérer le texte associé\n",
    "- segmenter en tokens\n",
    "- supprimer les urls\n",
    "- nettoyer les caractères inhérents à la structure d’un tweet\n",
    "- corriger les abréviations et les spécificités langagières des tweets à l’aide du dictionnaire DicoSlang (fichier SlangLookupTable.txt d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On commence par lire le fichier de tweets : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('testdata.csv', sep = ',', header=None,encoding='latin1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns=[\"score\",\"id\",\"date\",\"req\",\"user\",\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>req</th>\n",
       "      <th>user</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>Mon May 11 03:17:40 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>tpryan</td>\n",
       "      <td>@stellargirl I loooooooovvvvvveee my Kindle2. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Mon May 11 03:18:03 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>vcu451</td>\n",
       "      <td>Reading my kindle2...  Love it... Lee childs i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>Mon May 11 03:18:54 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>chadfu</td>\n",
       "      <td>Ok, first assesment of the #kindle2 ...it fuck...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>Mon May 11 03:19:04 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>SIX15</td>\n",
       "      <td>@kenburbary You'll love your Kindle2. I've had...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>Mon May 11 03:21:41 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>yamarama</td>\n",
       "      <td>@mikefish  Fair enough. But i have the Kindle2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   score  id                          date      req      user  \\\n",
       "0      4   3  Mon May 11 03:17:40 UTC 2009  kindle2    tpryan   \n",
       "1      4   4  Mon May 11 03:18:03 UTC 2009  kindle2    vcu451   \n",
       "2      4   5  Mon May 11 03:18:54 UTC 2009  kindle2    chadfu   \n",
       "3      4   6  Mon May 11 03:19:04 UTC 2009  kindle2     SIX15   \n",
       "4      4   7  Mon May 11 03:21:41 UTC 2009  kindle2  yamarama   \n",
       "\n",
       "                                             content  \n",
       "0  @stellargirl I loooooooovvvvvveee my Kindle2. ...  \n",
       "1  Reading my kindle2...  Love it... Lee childs i...  \n",
       "2  Ok, first assesment of the #kindle2 ...it fuck...  \n",
       "3  @kenburbary You'll love your Kindle2. I've had...  \n",
       "4  @mikefish  Fair enough. But i have the Kindle2...  "
      ]
     },
     "execution_count": 453,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On crée ensuite des copies de dataset pour une utilisation future "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_init=df.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp=df.copy()\n",
    "\n",
    "df_temp['content'] = df_temp['content'].apply(lambda x: re.sub('https?://[A-Za-z0-9./]+','',x))\n",
    "#df_temp['content'] = df_temp['content'].apply(lambda x: re.sub('([#])|([^a-zA-Z])',' ',x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Reading my kindle2...  Love it... Lee childs is good read.'"
      ]
     },
     "execution_count": 461,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temp.content[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prétraitement : ‘@’mention\n",
    "\n",
    "Maintenant , on va essayer de traiter les mentions @ . Bien que ces mentions contiennent des informations ( la personne qui  a tweeté ou qui concerné par le tweet ou ...) , cela n'est pas pertinent dans le modèle d'analyse de sentiment . De ce fait ,on va les supprimer en utilisant le regex \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avant de supprimer les \"@\" , essayant de récuperer le nombre de mentions \"@...\" dans tout le corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de mention '@' est 124\n"
     ]
    }
   ],
   "source": [
    "nb_att=sum(df['content'].apply(lambda x: len(re.findall(r'@[A-Za-z0-9]+', x))))\n",
    "print( \"Nombre de mention '@' est %s\" %nb_att)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C'est un peu étrange car on s'attendait à que le nombre de mentions de @ soit comparable au nombre de tweet ... Supprimons maintenant ces mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@stellargirl I loooooooovvvvvveee my Kindle2. Not that the DX is cool, but the 2 is fantastic in its own right.'"
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.content[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['content'] = df['content'].apply(lambda x: re.sub(r'@[A-Za-z0-9]+','',x))\n",
    "#[^a-zA-Z]\", \" \"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' I loooooooovvvvvveee my Kindle2. Not that the DX is cool, but the 2 is fantastic in its own right.'"
      ]
     },
     "execution_count": 467,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.content[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prétraitement: URL links\n",
    "Ensuite , on va supprimer les url links , ces url contiennent bien evidemment des informations mais qui ne sont pas utiles à l'analyse de sentiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['content'] = df['content'].apply(lambda x: re.sub('https?://[A-Za-z0-9./]+','',x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i love lebron. '"
      ]
     },
     "execution_count": 470,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.content[17]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prétraitement : Les hashtags\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commençons par calculer le nombre de hashtag dans le corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de hashtag  est 8477\n"
     ]
    }
   ],
   "source": [
    "nb_hash=sum(df['content'].apply(lambda x: len(re.findall(r'([#])|([^a-zA-Z])', x))))\n",
    "print( \"Nombre de hashtag  est %s\" %nb_hash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On s'attendait à que le nombre de hashtag soit aussi élévé car il fait partie des entités les plus trouvées dans les tweets . Malgré qu'ils peuvent apporter une information utile au sentiment , on préfère en premier lieu supprimer les hashtags pour facilier le traitement.Supprimons maintenant les Hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df['content'] = df['content'].apply(lambda x: re.sub('([#])|([^a-zA-Z])',' ',x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prétraitement : Double espace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite , on va supprimer les double space des tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ok first assesment of the kindle it fucking rocks '"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['content'] = df['content'].apply(lambda x: re.sub(' +', ' ',x))\n",
    "df.content[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calcul du nombre de caractère inhérents à la structure de tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici , on va supposer que le nombre d’occurrences des caractères inhérents à la structure de tweet est le nombre de hashtag plus nombre de mentions @ . De ce fait ce nombre est 8477+124 = **8601** caractères"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calcul du nombre hashtag\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le nombre de hashtag a été déjà calculer et il est égal à **8477**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prétraitement : correction des abréviations et des spécificités langagières des tweets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme on a mentionné au début de cette partie , pour améliorer le nettoyage du fichier , on va utiliser un dictionnaire contenant les corrections des abréviations des tweets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lecture des fichiers des abréviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictfile=\"SlangLookupTable.txt\"\n",
    "with open(dictfile) as f:\n",
    "    reader = csv.reader(f, delimiter=\"\\t\")\n",
    "    d = list(reader)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformation du fichier en dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = pd.DataFrame(d)\n",
    "df_dict.columns=['abv','mean']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abv</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>121</td>\n",
       "      <td>one to one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a/s/l</td>\n",
       "      <td>age, sex, location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>adn</td>\n",
       "      <td>any day now</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>afaik</td>\n",
       "      <td>as far as I know</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>afk</td>\n",
       "      <td>away from keyboard</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     abv                mean\n",
       "0    121          one to one\n",
       "1  a/s/l  age, sex, location\n",
       "2    adn         any day now\n",
       "3  afaik    as far as I know\n",
       "4    afk  away from keyboard"
      ]
     },
     "execution_count": 479,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dict.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant , on va créer une fonction `correcttabv` qui permet de reperer les abréviations dans les tweets ( soit à la fin , soit , au début , soit au milieu ) et les corriger "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "testtext=\"adn i do not know afk 121\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correctabv(text) :\n",
    "    for i in df_dict.abv :\n",
    "        text=text.replace(\" \"+ i+ \" \", \" \")\n",
    "        text=text.replace(\" \"+ i+ \",\", \" \")\n",
    "        text=text.replace(\" \"+ i+ \".\", \" \")\n",
    "        text=text.replace(\" \"+ i+ \"!\", \" \")\n",
    "        \n",
    "        if testtext.find(i)+len(i) ==len(text):\n",
    "            text=text.replace(i, \" \")\n",
    "        if testtext.find(i)==0:\n",
    "            text=text.replace(i, \" \")\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  i do not know  '"
      ]
     },
     "execution_count": 483,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correctabv(testtext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Appliquons maintenant cette fonction au dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['content'] =df['content'].apply(lambda x: correctabv(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prétraitment :  tokenize \n",
    "La tokenization est la tache de splitter les une chaines de caractères en plusieurs mots . Cela peut se faire via la fonction split classique , mais afin de tenir compte de nature grammaticale des mots et le contexte dans lequel il a été employé , il vaut mieux utiliser la fonction `word_tokenize` de nltk  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "preProcessedTweet = df['content'].apply(lambda x: nltk.word_tokenize(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [I, loooooooovvvvvveee, my, Kindle, Not, that,...\n",
       "1    [Reading, my, kindle, Love, it, Lee, childs, i...\n",
       "2    [Ok, first, assesment, of, the, kindle, it, fu...\n",
       "3    [You, ll, love, your, Kindle, I, ve, had, mine...\n",
       "4    [Fair, enough, But, i, have, the, Kindle, and,...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 492,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preProcessedTweet[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2- Etiquetage grammatical\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant on va s'interesser à la nature grammaticale des mots . Cela va nous servira après pour filtrer les mots .\n",
    "Pour ce faire, on va developper une fonction capable de déterminer la catégorie grammaticale (POS : Part Of Speech) de chaque mot du tweet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [(I, PRP), (loooooooovvvvvveee, VBP), (my, PRP...\n",
       "1    [(Reading, VBG), (my, PRP$), (kindle, NN), (Lo...\n",
       "2    [(Ok, NNP), (first, JJ), (assesment, NN), (of,...\n",
       "3    [(You, PRP), (ll, VBP), (love, VB), (your, PRP...\n",
       "4    [(Fair, NNP), (enough, RB), (But, CC), (i, NNS...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taggedTweet=preProcessedTweet.apply(lambda x: nltk.pos_tag(x))\n",
    "taggedTweet[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On développe ensuite une fonction permettant de compter le nombre de verbes dans un tweet : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "listverb_ind=[\"VB\",\"VBD\",\"VBG\",\"VBN\",\"VBP\",\"VBZ\"]\n",
    "\n",
    "def countverb(listtoken):\n",
    "    return np.sum([1 for i,j in listtoken if j in listverb_ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On affiche ensuite le nombre de verbes dans le corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de verbes est 1119.0\n"
     ]
    }
   ],
   "source": [
    "nb_verbs=sum(taggedTweet.apply(lambda x: countverb(x)))\n",
    "print( \"Nombre de verbes est %s\" %nb_verbs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 . Algorithme de détection v1 : appel au dictionnaire Sentiwordnet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cette partie , on a exploiter quelques fonctionalités intéressantes de NLTK : On utilisera la base de données wordnet qui permet d'accéder à l’ensemble des synsets qui sont liés à un mot donné à l’aide d’une commande simple sous Python. \n",
    "\n",
    "Dans cette étape , on fera les taches suivantes : \n",
    "- Récupération uniquement des mots correspondant à des adjectifs, noms, adverbes et verbes\n",
    "- Calculer pour chaque mot du tweet les scores associés à leur premier synset\n",
    "- Calculer pour chaque tweet la somme des scores positifs et négatifs des SentiSynsets du tweet,\n",
    "— Comparer la somme des scores positifs et des scores négatifs de chaque tweet pour décider de la\n",
    "classe à associer au tweet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour commencer , on définit une liste contenant les tag à garder dans le tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "listkeep=[\"JJ\",\"FW\",\"JJR\",\"JJS\",\"NN\",\"NNS\",\"NNP\",\"NNPS\",\"RB\",\"RBR\",\"RBS\",\"VB\",\"VBD\",\"VBG\",\"VBN\",\"VBP\",\"VBZ\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite , on définit une fonction `extractscoremot` qui renvoie les scores d'un mot donnée . Après on définit la fonction `scoretweet` qui prend comme input un tagwords de tweet , filtre les elements correspondant à {adj,noms,adv,verbes} , calcule le score de chaque mot et enfin renvoyer la somme des scores positifs et négatifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def extractscoremot (mot):\n",
    "    tt=wn.synsets(mot)\n",
    "    if len(wn.synsets(mot)) > 0 :\n",
    "        tt=tt[0].name() # à exploiter les autre options\n",
    "        s=swn.senti_synset(tt)\n",
    "        return s.pos_score(),s.neg_score()\n",
    "    else :\n",
    "        return 0,0\n",
    "\n",
    "def scoretweet (tweettag):\n",
    "    \n",
    "    listnouns=[ x for (x,y) in tweettag if y in listkeep]\n",
    "    return {'pos':np.sum([ extractscoremot(mot)[0] for mot in listnouns]),'neg':np.sum([ extractscoremot(mot)[1] for mot in listnouns])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un exemple d'output correspondant à un tweettags : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pos': 1.125, 'neg': 1.25}"
      ]
     },
     "execution_count": 497,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scoretweet(taggedTweet[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On applique ensuite cette fonction à notre corpus prétraité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "taggedTweetscore=taggedTweet.apply(lambda x: scoretweet(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On définit une fonction qui permettent de classer les tweet en {0,2,4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classscore(dictscore):\n",
    "    if dictscore['pos'] > dictscore['neg'] :\n",
    "        return [dictscore['pos'],dictscore['neg'],4]\n",
    "    elif dictscore['pos'] < dictscore['neg'] : return [dictscore['pos'],dictscore['neg'],0]\n",
    "    else : return  [dictscore['pos'],dictscore['neg'],2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "taggedTweetClass=taggedTweetscore.apply( lambda x: classscore(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'output de cette fonction est le suivant  :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [8.125, 1.25, 4]\n",
       "1    [6.625, 0.125, 4]\n",
       "2        [4.0, 7.0, 0]\n",
       "3       [14.0, 3.0, 4]\n",
       "4        [7.0, 2.0, 4]\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 501,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taggedTweetClass[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On récupérer ensuite la classe prédite pour chaque tweet et on calcule enfin la matrice de confusion :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "predctedscore=taggedTweetClass.apply(lambda x:x[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_confusion = pd.crosstab(df.score, predctedscore, rownames=['Actual'], colnames=['Predicted'], margins=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>2</th>\n",
       "      <th>4</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>65</td>\n",
       "      <td>7</td>\n",
       "      <td>105</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "      <td>112</td>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>167</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>90</td>\n",
       "      <td>24</td>\n",
       "      <td>384</td>\n",
       "      <td>498</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted   0   2    4  All\n",
       "Actual                     \n",
       "0          65   7  105  177\n",
       "2          15  12  112  139\n",
       "4          10   5  167  182\n",
       "All        90  24  384  498"
      ]
     },
     "execution_count": 504,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici on voit bien que le nombre de tweet positif correctement prédits est **167**  , ce qui est très moyen comme taux car on prédit beaucoup de positifs par rapports au réel ( 105 tweets neutres & 112 tweets négatifs sont classé positifs via notre algorithme) . On constate aussi que le nombre de tweet de classe 0  et dont la prédiction donne 4 est 105 , de ce fait , il y a un problème de distinction entre la classe neutre et la classe positive .On voit également qu'on a des erreurs de prédictions élévés sur les autres classes. Essayons de claculer la précision globale de l'algorithme ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le score de la première méthode est 0.49\n"
     ]
    }
   ],
   "source": [
    "score=(df_confusion[0:1][0].values[0] +df_confusion[1:2].values[0][1] +df_confusion[2:3].values[0][2])/df_confusion[3:4].values[0][3]\n",
    "print( \"Le score de la première méthode est %.2f\" %score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le score est très faible et il reste encore du travail à effectuer ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4- Algorithme de détection v2 : gestion de la négation et des modifieurs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cette partie , on va essayer de tenir compte des négations et des modifieurs dans le scoring des tweets\n",
    "On aura  besoin de : \n",
    "- La liste des mots en anglais correspondant à des négations \n",
    "- La liste des mots correspondant aux modifieurs \n",
    "\n",
    "On implémentera une fonctionqui ,pour chaque mot, effectue les opérations suivantes :\n",
    "- multiplie par 2 le score négatif et le score positif associés au mot si le mot précédent est un modifieur ;\n",
    "- utilise uniquement le score négatif du mot pour le score positif global du tweet et le score positif\n",
    "du mot pour le score négatif global du tweet si le mot précédent est une négation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On commence par lire le dictionnaire de modifiers et des négations et les transformer en dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>modifier</th>\n",
       "      <th>att</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>absolutely</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>definitely</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>extremely</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fuckin</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fucking</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     modifier att\n",
       "0  absolutely   1\n",
       "1  definitely   1\n",
       "2   extremely   2\n",
       "3      fuckin   2\n",
       "4     fucking   2"
      ]
     },
     "execution_count": 513,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictfile=\"BoosterWordList.txt\"\n",
    "\n",
    "with open(dictfile) as f:\n",
    "    reader = csv.reader(f, delimiter=\"\\t\")\n",
    "    d = list(reader)\n",
    "modifiers=pd.DataFrame(d)\n",
    "modifiers.columns=[\"modifier\",\"att\"]\n",
    "modifierslist=modifiers.modifier.tolist()   \n",
    "modifiers.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour les négations , on a pris les mots du dictionnaire fourni avec le TP et on les a mis dans une liste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "negatives=[\"aren't\",\"arent\",\"can't\",\"cannot\",\"cant\",\"don't\",\"dont\",\"isn't\",\"isnt\",\"never\",\"not\",\"won't\",\"wont\",\"wouldn't\",\"wouldnt\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On développe ensuite une fonction qui prend un mot et son prédecesseur et renvoie les scores positif et négatif de tweet en tenant compte des régles expliquées au début de la partie "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extractscoremotmodif (mot,prec):\n",
    "    tt=wn.synsets(mot)\n",
    "    coef=1\n",
    "    if len(wn.synsets(mot)) > 0 :\n",
    "        if prec in (modifierslist): \n",
    "            coef=2\n",
    "        tt=tt[0].name() # à exploiter les autre options\n",
    "        s=swn.senti_synset(tt)\n",
    "        if prec in negatives :\n",
    "            return coef*s.neg_score(),coef*s.pos_score(),1\n",
    "            \n",
    "        else :\n",
    "            return coef*s.pos_score(),coef*s.neg_score(),0\n",
    "    \n",
    "    else :\n",
    "        return 0,0,0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On implémente ensuite une fonction `scoretweetmodif` qui permet de scorer un tweettags en appelant la fonction précedement définie "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scoretweetmodif (tweettag):\n",
    "\n",
    "    scorepos=[]\n",
    "    listnouns=[ x for (x,y) in tweettag if y in listkeep]\n",
    "\n",
    "    for ind,mot in enumerate(listnouns) :\n",
    "        if ind > 0 :\n",
    "            score = extractscoremotmodif (mot,listnouns[ind-1])\n",
    "        else :\n",
    "            score =extractscoremot(mot)\n",
    "            score=(score[0],score[1],0)\n",
    "        scorepos.append(score)\n",
    "    return {'pos':np.sum([ mot[0] for mot in scorepos]),'neg':np.sum([mot[1] for mot in scorepos]),\n",
    " 'flag':np.sum([mot[2] for mot in scorepos])}\n",
    "   # return scorepos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On calcule ensuite le nouveau vecteur de score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [],
   "source": [
    "taggedTweetscore=taggedTweet.apply(lambda x: scoretweetmodif(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On récupère les termes négatifs contenus dans des tweets positifs dans une liste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "listneg=[ x for x in taggedTweetscore if (x['flag'] > 0 and x['pos'] > x ['neg']) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On calcule enfin le score et la matrice de confusion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "taggedTweetscore=taggedTweet.apply(lambda x: scoretweetmodif(x))\n",
    "taggedTweetClass=taggedTweetscore.apply( lambda x: classscore(x))\n",
    "predctedscore=taggedTweetClass.apply(lambda x:x[2])\n",
    "df_confusion = pd.crosstab(df.score, predctedscore, rownames=['Actual'], colnames=['Predicted'], margins=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>2</th>\n",
       "      <th>4</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>80</td>\n",
       "      <td>29</td>\n",
       "      <td>68</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19</td>\n",
       "      <td>58</td>\n",
       "      <td>62</td>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26</td>\n",
       "      <td>29</td>\n",
       "      <td>127</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>125</td>\n",
       "      <td>116</td>\n",
       "      <td>257</td>\n",
       "      <td>498</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted    0    2    4  All\n",
       "Actual                       \n",
       "0           80   29   68  177\n",
       "2           19   58   62  139\n",
       "4           26   29  127  182\n",
       "All        125  116  257  498"
      ]
     },
     "execution_count": 516,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le nombre de tweets negatifs ,neutres et positifs correctement\n",
    "détectés avec cette version de l’algorithme sont respectivement **80** , **58** et **127**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le score de cette version est 0.53\n"
     ]
    }
   ],
   "source": [
    "score=(df_confusion[0:1][0].values[0] +df_confusion[1:2].values[0][1] +df_confusion[2:3].values[0][2])/df_confusion[3:4].values[0][3]\n",
    "print(\"Le score de cette version est %.2f\" %score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On est arrivé à améliorer le score de 4% , ce qui n'est pas mal . On constate , via la matrice de confusion, que le traitement des négations et des modifieurs a permis de réduire l'erreur qu'on commet sur la classe 4 : L'algorithme a tendance au début de classifier les tweets en positifs , mais après ce traitement , il arrive à distinguer mieux le neutre et le négatif du positif . Essayons maintenant de creuser d'autres pistes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le nombre de termes négatifs contenus dans les tweets positifs est 4\n"
     ]
    }
   ],
   "source": [
    "listneg=[ x for x in taggedTweetscore if (x['flag'] > 0 and x['pos'] > x ['neg']) ]\n",
    "print( \"Le nombre de termes négatifs contenus dans les tweets positifs est %s\" %(len(listneg)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.  Algorithme de détection v3 : gestion des emoticons\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant , on va essayer de gérer les emoticons dans les tweets en utilisant le dictionnaire d’émoticons `EmoticonLookupTable.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va garder le vecteur de score obtenu avec la partie précédente et on contruisera également un autre score uniquement basé sur les emoticons , sur la base initiale de tweets , mais en supprimant les URL car elles contiennent `:/` .\n",
    "- Les émoticons positifs rencontrés augmentent de 1 la valeur du score positif du tweet\n",
    "- Les émoticons négatifs augmentent de 1 la valeur du score négatif du tweet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On commence par lire le fichier des emoticons et le transformer en dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dictfile=\"EmoticonLookupTable.txt\"\n",
    "with open(dictfile) as f:\n",
    "    reader = csv.reader(f, delimiter=\"\\t\")\n",
    "    d = list(reader)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoticon=pd.DataFrame(d)\n",
    "emoticon.columns=[\"symbol\",\"score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symbol</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>%-(</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>%-)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(-:</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(:</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(^ ^)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  symbol score\n",
       "0    %-(    -1\n",
       "1    %-)     1\n",
       "2    (-:     1\n",
       "3     (:     1\n",
       "4  (^ ^)     1"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emoticon.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On transforme la colonne score en numérique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoticon.score = pd.to_numeric(emoticon.score, errors='coerce')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On reparte de la base df_temp qui contient les tweets initiaux après avoir supprimer les urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>req</th>\n",
       "      <th>user</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>Mon May 11 03:17:40 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>tpryan</td>\n",
       "      <td>@stellargirl I loooooooovvvvvveee my Kindle2. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Mon May 11 03:18:03 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>vcu451</td>\n",
       "      <td>Reading my kindle2...  Love it... Lee childs i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>Mon May 11 03:18:54 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>chadfu</td>\n",
       "      <td>Ok, first assesment of the #kindle2 ...it fuck...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   score  id                          date      req    user  \\\n",
       "0      4   3  Mon May 11 03:17:40 UTC 2009  kindle2  tpryan   \n",
       "1      4   4  Mon May 11 03:18:03 UTC 2009  kindle2  vcu451   \n",
       "2      4   5  Mon May 11 03:18:54 UTC 2009  kindle2  chadfu   \n",
       "\n",
       "                                             content  \n",
       "0  @stellargirl I loooooooovvvvvveee my Kindle2. ...  \n",
       "1  Reading my kindle2...  Love it... Lee childs i...  \n",
       "2  Ok, first assesment of the #kindle2 ...it fuck...  "
      ]
     },
     "execution_count": 522,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temp.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On développe ensuite une fonction qui renvoie le score de chaque tweet en se basant uniquement sur les emoticons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getsmileyscore ( tweet) :\n",
    "    scorep,scoren=0,0\n",
    "    for i,emo in enumerate(emoticon.symbol.tolist()) :\n",
    "        if emo in tweet :\n",
    "            if int(emoticon.score[i]) > 0 :\n",
    "                scorep +=1\n",
    "            else : \n",
    "                scoren +=1 \n",
    "    return {'emop':scorep ,'emon':scoren}\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ci-desous un exemple d'output de la fonction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'emop': 0, 'emon': 1}"
      ]
     },
     "execution_count": 524,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet=\"Reading my kindle2... Love it... Lee childs i... :/\"\n",
    "getsmileyscore(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On calcule ensuite le emoticon_score de tous les tweets :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [],
   "source": [
    "smiley_score=df_temp.content.apply(lambda x: getsmileyscore(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On combine à la fin  le score de v2 et le score d'emoticon pour calcule rle score final :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [],
   "source": [
    "taggedTweetscoresmiley=taggedTweetscore.copy()\n",
    "for  i,el in enumerate(taggedTweetscore) :\n",
    "    c=taggedTweetscore[i].copy()\n",
    "    taggedTweetscoresmiley[i]['pos']=taggedTweetscoresmiley[i]['pos']+smiley_score[i]['emop']\n",
    "    taggedTweetscoresmiley[i]['neg']=taggedTweetscoresmiley[i]['neg']+smiley_score[i]['emon']\n",
    "   # if c['pos']> c['neg'] and  (taggedTweetscoresmiley[i]['pos'] < taggedTweetscoresmiley[i]['neg'] or taggedTweetscoresmiley[i]['pos']== taggedTweetscoresmiley[i]['neg']):\n",
    "    #    print([i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ci-dessous un extraint du vecteur de score obtenu : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     {'pos': 1.125, 'neg': 1.25, 'flag': 0}\n",
       "1    {'pos': 1.625, 'neg': 0.125, 'flag': 0}\n",
       "2        {'pos': 0.0, 'neg': 0.0, 'flag': 0}\n",
       "3        {'pos': 4.0, 'neg': 2.0, 'flag': 1}\n",
       "4        {'pos': 2.0, 'neg': 0.0, 'flag': 0}\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 529,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taggedTweetscoresmiley[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculons maintenant la nouvelle matrice de confusion et le score de l'algorithme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>2</th>\n",
       "      <th>4</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>88</td>\n",
       "      <td>27</td>\n",
       "      <td>62</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19</td>\n",
       "      <td>58</td>\n",
       "      <td>62</td>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26</td>\n",
       "      <td>24</td>\n",
       "      <td>132</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>133</td>\n",
       "      <td>109</td>\n",
       "      <td>256</td>\n",
       "      <td>498</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted    0    2    4  All\n",
       "Actual                       \n",
       "0           88   27   62  177\n",
       "2           19   58   62  139\n",
       "4           26   24  132  182\n",
       "All        133  109  256  498"
      ]
     },
     "execution_count": 531,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "taggedTweetClassSmiley=taggedTweetscoresmiley.apply( lambda x: classscore(x))\n",
    "predctedscoreSmiley=taggedTweetClassSmiley.apply(lambda x:x[2])\n",
    "df_confusion_smiley = pd.crosstab(df.score, predctedscoreSmiley, rownames=['Actual'], colnames=['Predicted'], margins=True)\n",
    "df_confusion_smiley"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le nombre de tweets negatifs ,neutres  et positifs correctement\n",
    "détectés avec cette version de l’algorithme sont respectivement **88** , **58** et **132**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le score de version V3 est 0.56\n"
     ]
    }
   ],
   "source": [
    "score=(df_confusion_smiley[0:1][0].values[0] +df_confusion_smiley[1:2].values[0][1] +df_confusion_smiley[2:3].values[0][2])/df_confusion_smiley[3:4].values[0][3]\n",
    "score\n",
    "print(\"Le score de version V3 est %.2f\" %score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le nombre d'emoticons  est 57\n"
     ]
    }
   ],
   "source": [
    "nbre_emoticon=np.sum([smiley['emop'] + smiley['emon'] for smiley in smiley_score])\n",
    "print(\"Le nombre d'emoticons  est %s\" %nbre_emoticon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le fait de tenir compte des emoticons nous a permis d'améliorer le score de 3 % . Etant donnée le nombre faible d'emoticons dans le corpus , on ne peut pas esperer une amélioration significative .\n",
    "On voit également qu'on commet une erreur importante dans la distinction entre la classe neutre et la classe positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 . Algorithme de détection v4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour améliorer notre score , on suggère d'envisager une approche sur deux étapes  : \n",
    "- La première  consiste à enrichir les dictionnaires de detection de sentiments et le traitmeent de ponctuation\n",
    "- La deuxième consiste à utiliser un corpus existant de tweet labélisé 'positf' et 'negatif' disponible sur intrernet , entrainer un classifieur la dessus , ensuite prédire le score de chaque tweet de notre dataset ( proba d'etre positif ) et enfin ajouter la probabilité ( si elle est très elevé ou très faible )  au score obtenu de l'algorithme v3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Première étape  : enrichissement et ajout des dicitionnaires "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En premier lieu , on a commencé par enrichir les dictionnaires de négations et de modifieurs utilisés précedemment vu qu'ils comportent un nombre faible d'élements . Pour ce faire : \n",
    "- on a mis à jour le dictionnaire des termes négatifs\n",
    "- on a ajouté un autre dictionnaire d'adverbes scrappé du https://en.wiktionary.org/wiki/Category:English_degree_adverbs\n",
    "- on a ajouté aussi les termes de ponctuations accentués comme '!!!' ou '???' qui pourrait avoir le meme effet que les adverbes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [],
   "source": [
    "negatives = [\"aint\", \"arent\", \"cannot\", \"cant\", \"couldnt\", \"darent\", \"didnt\", \"doesnt\",\n",
    "     \"ain't\", \"aren't\", \"can't\", \"couldn't\", \"daren't\", \"didn't\", \"doesn't\",\n",
    "     \"dont\", \"hadnt\", \"hasnt\", \"havent\", \"isnt\", \"mightnt\", \"mustnt\", \"neither\",\n",
    "     \"don't\", \"hadn't\", \"hasn't\", \"haven't\", \"isn't\", \"mightn't\", \"mustn't\",\n",
    "     \"neednt\", \"needn't\", \"never\", \"none\", \"nope\", \"nor\", \"not\", \"nothing\", \"nowhere\",\n",
    "     \"oughtnt\", \"shant\", \"shouldnt\", \"uhuh\", \"wasnt\", \"werent\",\n",
    "     \"oughtn't\", \"shan't\", \"shouldn't\", \"uh-uh\", \"wasn't\", \"weren't\",\n",
    "     \"without\", \"wont\", \"wouldnt\", \"won't\", \"wouldn't\", \"rarely\", \"seldom\", \"despite\",\"aren't\",\"arent\",\"can't\",\"cannot\",\"cant\",\"don't\",\"dont\",\"isn't\",\"isnt\",\"never\",\"not\",\"won't\",\n",
    "           \"wont\",\"wouldn't\",\"wouldnt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOOSTER_DICT = [\"absolutely\", \"amazingly\",\"awfully\", \"completely\", \"considerably\",\n",
    "     \"decidedly\", \"deeply\", \"effing\", \"enormously\",\n",
    "     \"entirely\", \"especially\", \"exceptionally\", \"extremely\",\n",
    "     \"fabulously\", \"flipping\", \"flippin\",\n",
    "     \"fricking\", \"frickin\", \"frigging\", \"friggin\", \"fully\", \"fucking\",\n",
    "     \"greatly\", \"hella\", \"highly\" , \"hugely\", \"incredibly\",\n",
    "     \"intensely\", \"majorly\", \"more\", \"most\", \"particularly\",\n",
    "     \"purely\", \"quite\", \"really\", \"remarkably\",\n",
    "     \"so\", \"substantially\",\n",
    "     \"thoroughly\", \"totally\", \"tremendously\",\n",
    "     \"uber\", \"unbelievably\", \"unusually\", \"utterly\",\n",
    "     \"very\",\"too\",\"quite\",\n",
    "     \"almost\", \"barely\", \"hardly\" , \"just enough\",\n",
    "     \"kind of\", \"kinda\", \"kindof\", \"kind-of\",\n",
    "     \"less\", \"little\", \"marginally\", \"occasionally\", \"partly\",\n",
    "     \"scarcely\", \"slightly\", \"somewhat\",\n",
    "     \"sort of\", \"sorta\", \"sortof\", \"sort-of\"]\n",
    "PUNC_LIST = [\"!!\", \"!!!\", \"??\", \"???\", \"?!?\", \"!?!\", \"?!?!\", \"!?!?\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a adapté ensuite nos fonctions de scoring pour qu'ils tiennent compte des nouveaux dictionnaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractscoremotmodif (mot,prec):\n",
    "    tt=wn.synsets(mot)\n",
    "    coef=1\n",
    "    if len(wn.synsets(mot)) > 0 :\n",
    "        if prec in (modifierslist+BOOSTER_DICT+PUNC_LIST): \n",
    "            coef=2\n",
    "        tt=tt[0].name() # à exploiter les autre options\n",
    "        s=swn.senti_synset(tt)\n",
    "        if prec in negatives :\n",
    "            return coef*s.neg_score(),coef*s.pos_score(),1\n",
    "            \n",
    "        else :\n",
    "            return coef*s.pos_score(),coef*s.neg_score(),0\n",
    "    \n",
    "    else :\n",
    "        return 0,0,0\n",
    "\n",
    "def scoretweetmodif (tweettag):\n",
    "\n",
    "    scorepos=[]\n",
    "    listnouns=[ x for (x,y) in tweettag if y in listkeep]\n",
    "\n",
    "    for ind,mot in enumerate(listnouns) :\n",
    "        if ind > 0 :\n",
    "            score = extractscoremotmodif (mot,listnouns[ind-1])\n",
    "        else :\n",
    "            score =extractscoremot(mot)\n",
    "            score=(score[0],score[1],0)\n",
    "        scorepos.append(score)\n",
    "    return {'pos':np.sum([ mot[0] for mot in scorepos]),'neg':np.sum([mot[1] for mot in scorepos]),\n",
    " 'flag':np.sum([mot[2] for mot in scorepos])}\n",
    "   # return scorepos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "####### une fonction qui calcule le score à partir d'une matrice de confusion  ############\n",
    "def getscore ( dfconfusion) :\n",
    "    score=(dfconfusion[0:1][0].values[0] +dfconfusion[1:2].values[0][1] +dfconfusion[2:3].values[0][2])/dfconfusion[3:4].values[0][3]\n",
    "    return score\n",
    "#taggedTweetscoremodif1=taggedTweet_beforv3.apply(lambda x: scoretweetmodif(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deuxième étape  : Modèle sur un corpus labélisé + et - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme on l'a précisé au début de la section , notre idée consiste à utiliser un jeux de données disponible sur internet qui contient des tweets labélisés { positif , négatif } , générer un embedding des tweets  et ensuite entrainer un classifieur permettant d'estimer la probabilité qu'un tweet soit positif. Ensuite on utilise ce classifieur pour prédire les classe de tweet de notre jeux de données ( en probabilité )  : \n",
    "- si la probabilité d'etre positif est supérieur à 0.7 , on ajoute un +1 au score positif du tweet obtenu de l'algorithme v3\n",
    "- si la probabilité d'etre positif est inférieur à 0.3 , on ajoute un +1 au score négatif obtenu de l'algorithme v3\n",
    "\n",
    "Le dataset 'Sentiment Analysis Dataset.csv' peut etre téléchargé via le lien suivant : http://thinknook.com/wp-content/uploads/2012/09/Sentiment-Analysis-Dataset.zip\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On commence par lire les données : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Sentiment Analysis Dataset.csv',\n",
    "                       usecols=['Sentiment', 'SentimentText'], error_bad_lines=False)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>or i just worry too much?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>Juuuuuuuuuuuuuuuuussssst Chillin!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>Sunny Again        Work Tomorrow  :-|  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>handed in my uniform today . i miss you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>hmmmm.... i wonder how she my number @-)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>I must think about positive..</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>thanks to all the haters up in my face a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>this weekend has sucked so far</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>jb isnt showing in australia any more!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>ok thats it you win.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>&amp;lt;-------- This is the way i feel right ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>awhhe man.... I'm completely useless rt no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>Feeling strangely fine. Now I'm gonna go l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>HUGE roll of thunder just now...SO scary!!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>I just cut my beard off. It's only been gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>Very sad about Iran.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>wompppp wompp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>You're the only one who can see this cause...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>&amp;lt;---Sad level is 3. I was writing a mass...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>...  Headed to Hospitol : Had to pull out o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentiment                                      SentimentText\n",
       "0           0                       is so sad for my APL frie...\n",
       "1           0                     I missed the New Moon trail...\n",
       "2           1                            omg its already 7:30 :O\n",
       "3           0            .. Omgaga. Im sooo  im gunna CRy. I'...\n",
       "4           0           i think mi bf is cheating on me!!!   ...\n",
       "5           0                  or i just worry too much?        \n",
       "6           1                 Juuuuuuuuuuuuuuuuussssst Chillin!!\n",
       "7           0         Sunny Again        Work Tomorrow  :-|  ...\n",
       "8           1        handed in my uniform today . i miss you ...\n",
       "9           1           hmmmm.... i wonder how she my number @-)\n",
       "10          0                      I must think about positive..\n",
       "11          1        thanks to all the haters up in my face a...\n",
       "12          0                     this weekend has sucked so far\n",
       "13          0             jb isnt showing in australia any more!\n",
       "14          0                               ok thats it you win.\n",
       "15          0      &lt;-------- This is the way i feel right ...\n",
       "16          0      awhhe man.... I'm completely useless rt no...\n",
       "17          1      Feeling strangely fine. Now I'm gonna go l...\n",
       "18          0       HUGE roll of thunder just now...SO scary!!!!\n",
       "19          0      I just cut my beard off. It's only been gr...\n",
       "20          0                               Very sad about Iran.\n",
       "21          0                                      wompppp wompp\n",
       "22          1      You're the only one who can see this cause...\n",
       "23          0     &lt;---Sad level is 3. I was writing a mass...\n",
       "24          0     ...  Headed to Hospitol : Had to pull out o..."
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici on définit quelques fonctions permettant de traiter les tweets de ce dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [],
   "source": [
    "def html2unicode(s):\n",
    "    # These are for regularizing HTML entities to Unicode:\n",
    "    html_entity_digit_re = re.compile(r\"&#\\d+;\")\n",
    "    html_entity_alpha_re = re.compile(r\"&\\w+;\")\n",
    "\n",
    "    # First the digits:\n",
    "    ents = set(html_entity_digit_re.findall(s))\n",
    "    if len(ents) > 0:\n",
    "        for ent in ents:\n",
    "            entnum = ent[2:-1]\n",
    "            try:\n",
    "                entnum = int(entnum)\n",
    "                s = s.replace(ent, unichr(entnum))\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    # Now the alpha versions:\n",
    "    ents = set(html_entity_alpha_re.findall(s))\n",
    "    ents = filter((lambda x : x != \"&amp;\"), ents)\n",
    "\n",
    "    for ent in ents:\n",
    "        entname = ent[1:-1]\n",
    "        try:\n",
    "            s = s.replace(ent, unichr(html.entities.name2codepoint[entname]))\n",
    "        except:\n",
    "            pass\n",
    "        s = s.replace(\"&amp;\", \" and \")\n",
    "\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code below was adapted from Chris Potts' implementation at\n",
    "# http://sentiment.christopherpotts.net/code-data/happyfuntokenizing.py\n",
    "\n",
    "import re\n",
    "#import htmlentitydefs\n",
    "import html.entities \n",
    "\n",
    "def get_regex_strings():\n",
    "    return (\n",
    "        # Phone numbers:\n",
    "        r\"\"\"\n",
    "        (?:\n",
    "          (?:            # (international)\n",
    "            \\+?[01]\n",
    "            [\\-\\s.]*\n",
    "          )?\n",
    "          (?:            # (area code)\n",
    "            [\\(]?\n",
    "            \\d{3}\n",
    "            [\\-\\s.\\)]*\n",
    "          )?\n",
    "          \\d{3}          # exchange\n",
    "          [\\-\\s.]*\n",
    "          \\d{4}          # base\n",
    "        )\"\"\"\n",
    "        ,\n",
    "        # Emoticons:\n",
    "        r\"\"\"\n",
    "        (?:\n",
    "          [<>]?\n",
    "          [:;=8]                     # eyes\n",
    "          [\\-o\\*\\']?                 # optional nose\n",
    "          [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth\n",
    "          |\n",
    "          [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth\n",
    "          [\\-o\\*\\']?                 # optional nose\n",
    "          [:;=8]                     # eyes\n",
    "          [<>]?\n",
    "        )\"\"\"\n",
    "        ,\n",
    "        # HTML tags:\n",
    "         r\"\"\"<[^>]+>\"\"\"\n",
    "        ,\n",
    "        # Twitter username:\n",
    "        r\"\"\"(?:@[\\w_]+)\"\"\"\n",
    "        ,\n",
    "        # Twitter hashtags:\n",
    "        r\"\"\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\"\"\"\n",
    "        ,\n",
    "        # Remaining word types:\n",
    "        r\"\"\"\n",
    "        (?:[a-z][a-z'\\-_]+[a-z])       # Words with apostrophes or dashes.\n",
    "        |\n",
    "        (?:[+\\-]?\\d+[,/.:-]\\d+[+\\-]?)  # Numbers, including fractions, decimals.\n",
    "        |\n",
    "        (?:[\\w_]+)                     # Words without apostrophes or dashes.\n",
    "        |\n",
    "        (?:\\.(?:\\s*\\.){1,})            # Ellipsis dots.\n",
    "        |\n",
    "        (?:\\S)                         # Everything else that isn't whitespace.\n",
    "        \"\"\"\n",
    "        )\n",
    "\n",
    "\n",
    "def html2unicode(s):\n",
    "    # These are for regularizing HTML entities to Unicode:\n",
    "    html_entity_digit_re = re.compile(r\"&#\\d+;\")\n",
    "    html_entity_alpha_re = re.compile(r\"&\\w+;\")\n",
    "\n",
    "    # First the digits:\n",
    "    ents = set(html_entity_digit_re.findall(s))\n",
    "    if len(ents) > 0:\n",
    "        for ent in ents:\n",
    "            entnum = ent[2:-1]\n",
    "            try:\n",
    "                entnum = int(entnum)\n",
    "                s = s.replace(ent, unichr(entnum))\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    # Now the alpha versions:\n",
    "    ents = set(html_entity_alpha_re.findall(s))\n",
    "    ents = filter((lambda x : x != \"&amp;\"), ents)\n",
    "\n",
    "    for ent in ents:\n",
    "        entname = ent[1:-1]\n",
    "        try:\n",
    "            s = s.replace(ent, unichr(html.entities.name2codepoint[entname]))\n",
    "        except:\n",
    "            pass\n",
    "        s = s.replace(\"&amp;\", \" and \")\n",
    "\n",
    "    return s\n",
    "\n",
    "\n",
    "def tokenize(s):\n",
    "    # This is the core tokenizing regex:\n",
    "    word_re = re.compile(r\"\"\"(%s)\"\"\" % \"|\".join(get_regex_strings()), re.VERBOSE | re.UNICODE)\n",
    "\n",
    "    # The emoticon string gets its own regex so that we can preserve case for them as needed:\n",
    "    emoticon_re = re.compile(get_regex_strings()[1], re.VERBOSE | re.I | re.UNICODE)\n",
    "\n",
    "    # Try to ensure unicode:\n",
    "\n",
    "\n",
    "    # Fix HTML character entitites:\n",
    "    s = html2unicode(s)\n",
    "\n",
    "    # Tokenize:\n",
    "    words = word_re.findall(s)\n",
    "\n",
    "    # Possible alter the case, but avoid changing emoticons like :D into :d:\n",
    "    words = map((lambda x : x if emoticon_re.search(x) else x.lower()), words)\n",
    "\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On génère ensuite un embedding  tf-idf des tweet prétraités  via la commande `TfidfVectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing tweet text...\n"
     ]
    }
   ],
   "source": [
    "print('Pre-processing tweet text...')\n",
    "corpus = data['SentimentText']\n",
    "vectorizer = TfidfVectorizer(decode_error='replace', strip_accents='unicode',\n",
    "                                 stop_words='english', tokenizer=tokenize)\n",
    "X = vectorizer.fit_transform(corpus.values)\n",
    "y = data['Sentiment'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On entraine ensuite un modèle **NaiveBayes** sur l'embedding des tweets  ( c'est le modèle basique , on pourra également essayer des approches plus sophistiquées comme les CNN et les RNN )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training sentiment classification model...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 556,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Training sentiment classification model...')\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le score du classifieur sur le train est 83% , on pourra certainement l'améliorer mais , par faute de temps , on ne va pas creuser ce point ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8324175510922873"
      ]
     },
     "execution_count": 557,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.score(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant , on recupère les données de notre dataset et on applique notre vectorizer pour générer l'embedding correspondant : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_predict = vectorizer.transform(df_temp.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On prédit ensuite la probabilité d'avoir un sentiment positif de chaque tweet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba_model=classifier.predict_proba(X_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5951628792743565,\n",
       " 0.6165334225115324,\n",
       " 0.6304053832021198,\n",
       " 0.6200921456994368,\n",
       " 0.6633596009306225,\n",
       " 0.6843895971415599,\n",
       " 0.11603542840026963,\n",
       " 0.7185028645747751,\n",
       " 0.765744695307406,\n",
       " 0.6402629234268521,\n",
       " 0.685392924249912,\n",
       " 0.4365668207043725,\n",
       " 0.6640085296999537,\n",
       " 0.47029888887596905,\n",
       " 0.2004342063906284,\n",
       " 0.6183404144764137,\n",
       " 0.20346297238884145,\n",
       " 0.47563211725327753,\n",
       " 0.41368438115792266,\n",
       " 0.5285190717810705,\n",
       " 0.6826535724485436,\n",
       " 0.6518773687300844,\n",
       " 0.5368696261540217,\n",
       " 0.45841869516094863,\n",
       " 0.3187564600924987,\n",
       " 0.888466570555584,\n",
       " 0.5043652116971936,\n",
       " 0.7927869444936917,\n",
       " 0.7621391251215481,\n",
       " 0.7256761312331165,\n",
       " 0.5598433590332862,\n",
       " 0.7313032057001897,\n",
       " 0.6155361288957714,\n",
       " 0.30317204305859086,\n",
       " 0.43398149971390554,\n",
       " 0.21932919592317157,\n",
       " 0.04353630944826192,\n",
       " 0.2327365586202071,\n",
       " 0.6244935318782446,\n",
       " 0.5402898620329991,\n",
       " 0.6347944149261476,\n",
       " 0.49715395595128664,\n",
       " 0.06262348325185872,\n",
       " 0.17921457385602513,\n",
       " 0.25737920418891813,\n",
       " 0.42556571916782704,\n",
       " 0.5000500885211578,\n",
       " 0.5299301142153201,\n",
       " 0.3935807032576889,\n",
       " 0.2894044798335682,\n",
       " 0.23357531723396374,\n",
       " 0.5530415931995838,\n",
       " 0.552118173634583,\n",
       " 0.6371628946666724,\n",
       " 0.46250512794004467,\n",
       " 0.7759123848469365,\n",
       " 0.8174662760592479,\n",
       " 0.7170977242730162,\n",
       " 0.6760449559620721,\n",
       " 0.2884178617106853,\n",
       " 0.5117126678225017,\n",
       " 0.3399490168695719,\n",
       " 0.6606903934421998,\n",
       " 0.4535577797615171,\n",
       " 0.27373020100265527,\n",
       " 0.4191448636514004,\n",
       " 0.8062699037000289,\n",
       " 0.7625204447184881,\n",
       " 0.5150101865196868,\n",
       " 0.5790034196404823,\n",
       " 0.6113484400521876,\n",
       " 0.5678804711808574,\n",
       " 0.33543070289828697,\n",
       " 0.7506517175041125,\n",
       " 0.6545827526001047,\n",
       " 0.36887185616059504,\n",
       " 0.6810349488641563,\n",
       " 0.6427983475805169,\n",
       " 0.36234169178404346,\n",
       " 0.25284264122375527,\n",
       " 0.7731174487192253,\n",
       " 0.46835689740601627,\n",
       " 0.8057117329420683,\n",
       " 0.4322594046792255,\n",
       " 0.7318470364737665,\n",
       " 0.4975809813178676,\n",
       " 0.2693404657078242,\n",
       " 0.4655550709894883,\n",
       " 0.17267569963149162,\n",
       " 0.33653928231987873,\n",
       " 0.30101923480846043,\n",
       " 0.2650092893015356,\n",
       " 0.07425102671095613,\n",
       " 0.30424601975215654,\n",
       " 0.1631329934811582,\n",
       " 0.4180157951518258,\n",
       " 0.09484827878651994,\n",
       " 0.07243766360864863,\n",
       " 0.2782735485664142,\n",
       " 0.11349079493286622,\n",
       " 0.5630230371066024,\n",
       " 0.7849654049128679,\n",
       " 0.4675287300129609,\n",
       " 0.466881063203154,\n",
       " 0.24074224224452648,\n",
       " 0.12575354701953106,\n",
       " 0.6738405288691457,\n",
       " 0.6637590560392538,\n",
       " 0.746863146002676,\n",
       " 0.7734077564520687,\n",
       " 0.8346844932511489,\n",
       " 0.7107545619702547,\n",
       " 0.7570172148761154,\n",
       " 0.615525727246594,\n",
       " 0.7135424187372947,\n",
       " 0.6860216765338568,\n",
       " 0.6557059703893084,\n",
       " 0.6240635787374716,\n",
       " 0.6216409911876895,\n",
       " 0.6978222551455738,\n",
       " 0.7480322216091324,\n",
       " 0.7388716453957522,\n",
       " 0.6969540240905843,\n",
       " 0.2137476068428649,\n",
       " 0.7633407114372281,\n",
       " 0.7752902002957013,\n",
       " 0.6892581120974363,\n",
       " 0.7374424251180098,\n",
       " 0.6655492092380779,\n",
       " 0.6623099238802704,\n",
       " 0.5911944195407057,\n",
       " 0.7001932478425229,\n",
       " 0.7317319835129087,\n",
       " 0.7742540733806127,\n",
       " 0.7135683131086331,\n",
       " 0.6829910172931908,\n",
       " 0.5029378229521777,\n",
       " 0.4626784197270908,\n",
       " 0.22735139993677211,\n",
       " 0.31024066097571873,\n",
       " 0.2791192247839125,\n",
       " 0.1357412475161148,\n",
       " 0.20199552963585174,\n",
       " 0.05251157122940075,\n",
       " 0.1413533629138166,\n",
       " 0.2015388192414805,\n",
       " 0.1803968292064523,\n",
       " 0.11762677693703401,\n",
       " 0.17130469153706665,\n",
       " 0.12012059772966376,\n",
       " 0.34548755667130043,\n",
       " 0.2322031159684297,\n",
       " 0.051789153895156294,\n",
       " 0.5453354733401312,\n",
       " 0.34841888219053874,\n",
       " 0.4123220280682164,\n",
       " 0.4490027249622885,\n",
       " 0.38830973693873666,\n",
       " 0.4920949389036155,\n",
       " 0.47160100615545963,\n",
       " 0.4730042617775219,\n",
       " 0.1140708844821395,\n",
       " 0.07344498526063753,\n",
       " 0.2938055291571723,\n",
       " 0.07540442143753351,\n",
       " 0.3948144757620059,\n",
       " 0.33377034948161494,\n",
       " 0.19578702357310943,\n",
       " 0.12519600200650996,\n",
       " 0.20482436133495655,\n",
       " 0.20699383265232307,\n",
       " 0.3847354669107471,\n",
       " 0.37867986901546324,\n",
       " 0.5304465753326759,\n",
       " 0.37269938344791,\n",
       " 0.4306663721866257,\n",
       " 0.2806922144298348,\n",
       " 0.4086981427436494,\n",
       " 0.699058692766329,\n",
       " 0.6126995524510293,\n",
       " 0.407654616014574,\n",
       " 0.43621145179423,\n",
       " 0.4476726194624092,\n",
       " 0.4743392982293291,\n",
       " 0.360757792160958,\n",
       " 0.38700589996243157,\n",
       " 0.5587629914487007,\n",
       " 0.6587799149637192,\n",
       " 0.6822966176319641,\n",
       " 0.6103206238641253,\n",
       " 0.6606897040125711,\n",
       " 0.6873995484240546,\n",
       " 0.627417210220911,\n",
       " 0.5544520644299138,\n",
       " 0.6370910395672079,\n",
       " 0.5242021888725347,\n",
       " 0.717966112277888,\n",
       " 0.7387350951280276,\n",
       " 0.5095093823212647,\n",
       " 0.5577822679955095,\n",
       " 0.5325369859148813,\n",
       " 0.5957108770833612,\n",
       " 0.7012221655687372,\n",
       " 0.6880378768904927,\n",
       " 0.3897250148246523,\n",
       " 0.6222460686607539,\n",
       " 0.7309881012373453,\n",
       " 0.6759030365784948,\n",
       " 0.525166114133204,\n",
       " 0.510269072875708,\n",
       " 0.22155819737024346,\n",
       " 0.13684928129763724,\n",
       " 0.4853132263049519,\n",
       " 0.15860971115045708,\n",
       " 0.13394467139368807,\n",
       " 0.4243792541171926,\n",
       " 0.632983182430268,\n",
       " 0.24561281160342688,\n",
       " 0.3400376404359438,\n",
       " 0.20725290557658388,\n",
       " 0.5545179326470231,\n",
       " 0.08242323902532932,\n",
       " 0.24330585183099926,\n",
       " 0.22484272398033958,\n",
       " 0.2331654065815157,\n",
       " 0.12735969174531236,\n",
       " 0.49635998090947925,\n",
       " 0.2650250254759543,\n",
       " 0.1965293656790886,\n",
       " 0.249826402250685,\n",
       " 0.062236477144318875,\n",
       " 0.4005793638706687,\n",
       " 0.7264952023909786,\n",
       " 0.6707044011175926,\n",
       " 0.3248268291527369,\n",
       " 0.3991041910126962,\n",
       " 0.8455746152862701,\n",
       " 0.5961985640592965,\n",
       " 0.7053000216589895,\n",
       " 0.7700176938891441,\n",
       " 0.3968118044134541,\n",
       " 0.43196552748673595,\n",
       " 0.6314817149772379,\n",
       " 0.8271508102591253,\n",
       " 0.7266879137800711,\n",
       " 0.7107393923846198,\n",
       " 0.7805770583703263,\n",
       " 0.4433365191837099,\n",
       " 0.8245262525597689,\n",
       " 0.7822908948024843,\n",
       " 0.23292254204289592,\n",
       " 0.4290327697932566,\n",
       " 0.6042649743048412,\n",
       " 0.6588231230042351,\n",
       " 0.441321031333753,\n",
       " 0.47609477374057363,\n",
       " 0.5073536603773947,\n",
       " 0.5910895796486095,\n",
       " 0.4398300065540129,\n",
       " 0.553205145741692,\n",
       " 0.512780743160208,\n",
       " 0.5525032443665012,\n",
       " 0.4773770261794203,\n",
       " 0.5130939241521856,\n",
       " 0.6741845909854445,\n",
       " 0.4744404339244405,\n",
       " 0.5964649895588253,\n",
       " 0.5366685273291221,\n",
       " 0.7687456755567351,\n",
       " 0.3509388239005154,\n",
       " 0.6657450437766822,\n",
       " 0.1063924201645818,\n",
       " 0.1056912901649402,\n",
       " 0.7038315697922568,\n",
       " 0.5440344803465789,\n",
       " 0.7577729691033153,\n",
       " 0.42220239981700264,\n",
       " 0.7672802454821952,\n",
       " 0.7262730247509566,\n",
       " 0.5463247292701396,\n",
       " 0.1879437765687972,\n",
       " 0.18112651208303004,\n",
       " 0.46265415027440177,\n",
       " 0.39757861308938436,\n",
       " 0.5210258074150597,\n",
       " 0.7080683273248781,\n",
       " 0.328783560915837,\n",
       " 0.7757103027329113,\n",
       " 0.6838289182584071,\n",
       " 0.7558132740911715,\n",
       " 0.8100222819534464,\n",
       " 0.7508630439598019,\n",
       " 0.5278737202510387,\n",
       " 0.5067361859594647,\n",
       " 0.7916199777397952,\n",
       " 0.5354605847899053,\n",
       " 0.046750708097724275,\n",
       " 0.09906348336918724,\n",
       " 0.42803128752441394,\n",
       " 0.1839227682524785,\n",
       " 0.4133985464733102,\n",
       " 0.17495918717058137,\n",
       " 0.3291060590170251,\n",
       " 0.4866673176966451,\n",
       " 0.1940069120990123,\n",
       " 0.37093105363533374,\n",
       " 0.7039609816470209,\n",
       " 0.45006220483764064,\n",
       " 0.5937458903482552,\n",
       " 0.6086824762390479,\n",
       " 0.6628057164929153,\n",
       " 0.6261118686796424,\n",
       " 0.29727832498021833,\n",
       " 0.7071687009758137,\n",
       " 0.19002378159449787,\n",
       " 0.9224127398086459,\n",
       " 0.6810245072546369,\n",
       " 0.798607594044597,\n",
       " 0.8186904774749,\n",
       " 0.6781303860677775,\n",
       " 0.5657466863460818,\n",
       " 0.5341195602658921,\n",
       " 0.7700277061815647,\n",
       " 0.7721723938140591,\n",
       " 0.6302966644263356,\n",
       " 0.5482847667706573,\n",
       " 0.747426360064437,\n",
       " 0.5178735956715609,\n",
       " 0.3424801800822271,\n",
       " 0.5072218899455395,\n",
       " 0.40170374285365695,\n",
       " 0.5103769121955484,\n",
       " 0.2934952968459115,\n",
       " 0.29368287841624524,\n",
       " 0.047578490173589345,\n",
       " 0.6389963850911435,\n",
       " 0.2820618359037801,\n",
       " 0.7584582702402058,\n",
       " 0.5550184623675756,\n",
       " 0.7363762401258831,\n",
       " 0.46850938310980145,\n",
       " 0.5040500242061446,\n",
       " 0.5246181725527984,\n",
       " 0.20055788511035766,\n",
       " 0.6827046588280982,\n",
       " 0.5875068073642246,\n",
       " 0.8354095214176629,\n",
       " 0.9003236755616569,\n",
       " 0.6964617648132484,\n",
       " 0.8512393378228118,\n",
       " 0.5133377353086939,\n",
       " 0.6237517848139007,\n",
       " 0.807557714346441,\n",
       " 0.61188398702675,\n",
       " 0.615363306456297,\n",
       " 0.7582701742270852,\n",
       " 0.8748526879795454,\n",
       " 0.822997934919841,\n",
       " 0.68299972614309,\n",
       " 0.6788715319653305,\n",
       " 0.6468687012148588,\n",
       " 0.5083344085537586,\n",
       " 0.7504629818947074,\n",
       " 0.8979129517127402,\n",
       " 0.6611141018499743,\n",
       " 0.7496311441892165,\n",
       " 0.5786174790700616,\n",
       " 0.8025938200828603,\n",
       " 0.678690802290243,\n",
       " 0.7788850839131568,\n",
       " 0.6385929091610673,\n",
       " 0.4443076885841105,\n",
       " 0.8163837829037889,\n",
       " 0.18481717712364207,\n",
       " 0.32006422603920276,\n",
       " 0.5611440799784445,\n",
       " 0.3165636279154283,\n",
       " 0.3226507767735259,\n",
       " 0.4813265798614263,\n",
       " 0.4001208348992653,\n",
       " 0.44867229260034625,\n",
       " 0.1808007554433471,\n",
       " 0.33824706638622737,\n",
       " 0.09819868664139687,\n",
       " 0.08987919066428686,\n",
       " 0.10871944286669562,\n",
       " 0.4431720490854325,\n",
       " 0.30827236546502457,\n",
       " 0.22135571250656946,\n",
       " 0.3990112202797444,\n",
       " 0.17900642419451973,\n",
       " 0.16429796737821903,\n",
       " 0.08932487694878402,\n",
       " 0.23405862891774148,\n",
       " 0.10172377302687811,\n",
       " 0.3086413768345874,\n",
       " 0.36782718401921116,\n",
       " 0.15986181205265534,\n",
       " 0.19675170904335382,\n",
       " 0.5789352649889298,\n",
       " 0.4722830349740498,\n",
       " 0.5489669413383168,\n",
       " 0.3083861672272533,\n",
       " 0.24998459586194846,\n",
       " 0.34981624047339227,\n",
       " 0.3248139273449888,\n",
       " 0.08543440196380497,\n",
       " 0.3886434671789644,\n",
       " 0.2386803958745216,\n",
       " 0.20158387170550268,\n",
       " 0.22453522402760406,\n",
       " 0.4664611295785693,\n",
       " 0.27481083204403406,\n",
       " 0.5822454266701909,\n",
       " 0.6275997189134324,\n",
       " 0.4547516346781244,\n",
       " 0.46844291140662603,\n",
       " 0.11969487749745908,\n",
       " 0.3906247835874505,\n",
       " 0.7084208285968192,\n",
       " 0.2163012384489504,\n",
       " 0.40263841976354087,\n",
       " 0.3820750419019757,\n",
       " 0.6636580734678469,\n",
       " 0.5871658638548239,\n",
       " 0.4102255234302867,\n",
       " 0.4239389271932645,\n",
       " 0.5899358120432026,\n",
       " 0.5963282334130859,\n",
       " 0.3311779873714889,\n",
       " 0.7123225072560115,\n",
       " 0.5958494803020384,\n",
       " 0.7313241420632404,\n",
       " 0.5773497619001292,\n",
       " 0.5817210707817758,\n",
       " 0.8030657448821404,\n",
       " 0.43545326813553836,\n",
       " 0.5739768815435154,\n",
       " 0.6243746000838994,\n",
       " 0.5077431751480933,\n",
       " 0.2745368696988524,\n",
       " 0.5755071047478064,\n",
       " 0.5775851055139678,\n",
       " 0.6300096258624259,\n",
       " 0.5838674916250357,\n",
       " 0.5967776974126694,\n",
       " 0.6613581549100018,\n",
       " 0.5900042984137857,\n",
       " 0.5763943182204346,\n",
       " 0.7277298636827203,\n",
       " 0.356078412926805,\n",
       " 0.7804815451880597,\n",
       " 0.6362906205490358,\n",
       " 0.15530217115845726,\n",
       " 0.6978463639554513,\n",
       " 0.4920224970711249,\n",
       " 0.566782815572941,\n",
       " 0.6129609849189132,\n",
       " 0.33915007702590677,\n",
       " 0.5829910547715738,\n",
       " 0.36168130664293724,\n",
       " 0.8949091533868413,\n",
       " 0.3795584595656294,\n",
       " 0.7939273183767853,\n",
       " 0.3391216738293966,\n",
       " 0.455790819734106,\n",
       " 0.6034469421636608,\n",
       " 0.8825109379691077,\n",
       " 0.10603601991617813,\n",
       " 0.4871708338816775,\n",
       " 0.617873415092733,\n",
       " 0.8833348644621243,\n",
       " 0.30916189115180054,\n",
       " 0.4798442100994261,\n",
       " 0.282876705697608,\n",
       " 0.6548721148790241,\n",
       " 0.7971147126311728,\n",
       " 0.6047847304887485,\n",
       " 0.7587767172368242,\n",
       " 0.46421582317413657,\n",
       " 0.35075786785032204,\n",
       " 0.310354765053522,\n",
       " 0.28709300580529834,\n",
       " 0.40020716894044867,\n",
       " 0.1588533327814174,\n",
       " 0.2769976759024841,\n",
       " 0.5859503295749806,\n",
       " 0.7278098634806179,\n",
       " 0.7815262950740766,\n",
       " 0.7716822170564785,\n",
       " 0.41232739127367235,\n",
       " 0.7344134172843805,\n",
       " 0.3801833418415178,\n",
       " 0.6409141385838697,\n",
       " 0.2263465557772004,\n",
       " 0.6023047215968794,\n",
       " 0.10584501499550296,\n",
       " 0.10176987054791772]"
      ]
     },
     "execution_count": 561,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_model_pos=[ i[1] for i in proba_model]\n",
    "score_model_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On recupère après les scores des tweets de notre dernier algorithme du TP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "taggedTweetscoresmileymodel=taggedTweetscoresmiley.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On met à jour les scores en tenant compte de la probabilité issu du modèle NaiveBayes : \n",
    "- si la probabilité d'etre positif est supérieur à 0.7 , on ajoute un +1 au score positif du tweet obtenu de l'algorithme v3\n",
    "- si la probabilité d'etre positif est inférieur à 0.3 , on ajoute un +1 au score négatif obtenu de l'algorithme v3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [],
   "source": [
    "#taggedTweetClassSmiley=taggedTweetscoresmiley.apply( lambda x: classscore(x))\n",
    "for i,s in enumerate(taggedTweetscoresmiley) :\n",
    "    if score_model_pos[i] > 0.7 :\n",
    "        taggedTweetscoresmileymodel[i]['pos']=taggedTweetscoresmiley[i]['pos']+1\n",
    "    \n",
    "    if score_model_pos[i] < 0.3 :\n",
    "        taggedTweetscoresmileymodel[i]['neg']=taggedTweetscoresmiley[i]['neg']+1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On calcule enfin la matrice de confusion  et le score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>2</th>\n",
       "      <th>4</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>132</td>\n",
       "      <td>10</td>\n",
       "      <td>35</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>52</td>\n",
       "      <td>67</td>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17</td>\n",
       "      <td>13</td>\n",
       "      <td>152</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>169</td>\n",
       "      <td>75</td>\n",
       "      <td>254</td>\n",
       "      <td>498</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted    0   2    4  All\n",
       "Actual                      \n",
       "0          132  10   35  177\n",
       "2           20  52   67  139\n",
       "4           17  13  152  182\n",
       "All        169  75  254  498"
      ]
     },
     "execution_count": 563,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taggedTweetClassSmileyModel=taggedTweetscoresmileymodel.apply( lambda x: classscore(x))\n",
    "predctedscoreSmileymodel=taggedTweetClassSmileyModel.apply(lambda x:x[2])\n",
    "df_confusion_smileymodel = pd.crosstab(df.score, predctedscoreSmileymodel, rownames=['Actual'], colnames=['Predicted'], margins=True)\n",
    "df_confusion_smileymodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le score du dernier algo est 0.67\n"
     ]
    }
   ],
   "source": [
    "score=(df_confusion_smileymodel[0:1][0].values[0] +df_confusion_smileymodel[1:2].values[0][1] +df_confusion_smileymodel[2:3].values[0][2])/df_confusion_smileymodel[3:4].values[0][3]\n",
    "score\n",
    "print( \"Le score du dernier algo est %.2f\" %score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On arrive à améliorer le score initial de presque 10% , ce qui n'est pas mal de tout par rapport à l'effort fourni sur la partie modélisation .On arrive surtout à distinguer le neutre du positif via cette approche ( dans v3 , il y avait 62 neutres qui sont classés positifs , et maintenant ça devient 32 ) .\n",
    "Pour continuer à améliorer le score , on pourra enrichir le dictionnaires des emoticons , utiliser des dictionnaires existant des mots +/- , des traducteurs de phrases ( parfois on peut observer des mots en langues étrangères ) , des termes anglais courants ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
